{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "86f49ea4",
      "metadata": {
        "id": "86f49ea4"
      },
      "source": [
        "# Assignment 5 - 70 points possible\n",
        "\n",
        "## Apple, Bird, Cookie:\n",
        "## Data from Google's Quick, Draw!  Game"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c52a6d9",
      "metadata": {
        "id": "5c52a6d9"
      },
      "source": [
        "We will perform 2 tasks in this homework.  The first is to build a classifier that can tell whether a drawing is of an apple, a bird, or a cookie.  The data is taken from a game you can play online, developed by Google, called Quick, Draw!  The drawings are all Pictionary-style quick sketches of things.  Google has already done the work of turning the drawings into grayscale 28 x 28 images that are good for machine learning.  Since the images don't have that many pixels, the drawings are simplified, and there is a lot of data, this is an example of a relatively easy task for machine learning generally and neural networks in particular.  (There are many similar datasets here:  https://console.cloud.google.com/storage/browser/quickdraw_dataset/full/numpy_bitmap;tab=objects?pli=1&prefix=&forceOnObjectsSortingFiltering=false)\n",
        "\n",
        "Before you do anything else in this assignment, it's *strongly recommended* that you do two things with your Google Colab setup.  First, go to the upper right and select \"Connect to a hosted runtime.\"  Second, go to the menu at the top and select Runtime->Change runtime type->Pick GPU.  Selecting \"GPU\" will dramatically speed up the training times for the neural networks in this assignment.  But, picking GPU also resets the runtime, which is why you should do this before anything else.\n",
        "\n",
        "Download the two zip files associated with this homework at https://drive.google.com/file/d/14ZkNqKC34mUW5yUa6WjYWf1R-CempaoB/view?usp=sharing and https://drive.google.com/file/d/14UmGyFC_WSywNcm2yrCfe0x04IuA42O8/view?usp=sharing and place them in your own Google Drive.  Then run the code boxes below (possibly modifying the path) to mount the drive and unzip the files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "MpMvOAecgMDQ",
      "metadata": {
        "id": "MpMvOAecgMDQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08289dda-44ad-4364-9c31-ecef903bbbe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "sMkp4tFNuoKx",
      "metadata": {
        "id": "sMkp4tFNuoKx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1ddc51e-5bb3-4021-d06f-627e8ad433b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  gdrive/MyDrive/Junior Year/DS 340/HW 5/apple_bird_cookie.zip\n",
            "  inflating: full_numpy_bitmap_apple.npy  \n",
            "  inflating: __MACOSX/._full_numpy_bitmap_apple.npy  \n",
            "  inflating: full_numpy_bitmap_bird.npy  \n",
            "  inflating: __MACOSX/._full_numpy_bitmap_bird.npy  \n",
            "  inflating: full_numpy_bitmap_cookie.npy  \n",
            "  inflating: __MACOSX/._full_numpy_bitmap_cookie.npy  \n"
          ]
        }
      ],
      "source": [
        "# You may need to change \"NNAssign\" to your own directory name\n",
        "!unzip gdrive/MyDrive/'Junior Year'/'DS 340'/'HW 5'/apple_bird_cookie.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3df7fbe2",
      "metadata": {
        "id": "3df7fbe2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "apples = np.load('full_numpy_bitmap_apple.npy')\n",
        "birds = np.load('full_numpy_bitmap_bird.npy')\n",
        "cookies = np.load('full_numpy_bitmap_cookie.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "215adfc5",
      "metadata": {
        "id": "215adfc5"
      },
      "source": [
        "(1, 2pts) Call np.concatenate to join the three datasets together into a single array called \"all_kinds\".  Notice that the three datasets should be passed to np.concatenate as a tuple.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c687d765",
      "metadata": {
        "id": "c687d765"
      },
      "outputs": [],
      "source": [
        "# TODO concatenate the data\n",
        "all_kinds = np.concatenate((apples, birds, cookies))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2bb4409",
      "metadata": {
        "id": "a2bb4409"
      },
      "source": [
        "(2, 3 pts) Now we need a list of labels that is as long as all_kinds.  Create a list named \"labels\" that is as long as all_kinds, where each element identifies which kind of drawing can be found at that place in all_kinds.  Label apples as 0, birds as 1, and cookies as 2.  (You may find it useful to call len() on the apples, birds, and cookies arrays.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9eaadef5",
      "metadata": {
        "id": "9eaadef5"
      },
      "outputs": [],
      "source": [
        "# TODO make labels\n",
        "labels = [0] * len(apples) + [1] * len(birds) + [2] * len(cookies)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16c9a7f4",
      "metadata": {
        "id": "16c9a7f4"
      },
      "source": [
        "Now we'll call train_test_split to separate the data into training and testing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d1c6e971",
      "metadata": {
        "id": "d1c6e971"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(all_kinds, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49f0afaa",
      "metadata": {
        "id": "49f0afaa"
      },
      "source": [
        "(3, 2pts) This data ranges from 0 to 255, but neural networks tend to work best when the data is between 0 and 1.\n",
        "Scale the train and test data by dividing it by 255."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "378d00d0",
      "metadata": {
        "id": "378d00d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26e6ba2c-e450-415a-e0ac-d23e7615cbf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(307235, 784)\n"
          ]
        }
      ],
      "source": [
        "# TODO scale the data\n",
        "print(x_train.shape)\n",
        "x_train = x_train/255\n",
        "x_test = x_test/255"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcf559ba",
      "metadata": {
        "id": "dcf559ba"
      },
      "source": [
        "(4, 5pts) If we examine the shape of x, it isn't quite what we want yet - it is an array of arrays that are of size length-of-data x 784.  784 is 28x28, and we want our convolutional neural networks to perceive the data as a 28x28 square instead of one long array of 784 elements.  Call x_train.reshape() and x_test.reshape() so that their dimensions are length-of-data x 28 x 28.  (Note that reshape expects a tuple that is the size as its argument.  You can pass -1 as one of the dimensions if you don't want to figure out how long the array is.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c6c77691",
      "metadata": {
        "id": "c6c77691",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b97b7224-6950-42eb-cc91-e0e14e969cf2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(784,)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "x_train[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ec1618dd",
      "metadata": {
        "id": "ec1618dd"
      },
      "outputs": [],
      "source": [
        "# TODO reshape x_train\n",
        "x_train = x_train.reshape((-1, 28,28))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "06703e7c",
      "metadata": {
        "id": "06703e7c"
      },
      "outputs": [],
      "source": [
        "# TODO reshape x_test\n",
        "x_test = x_test.reshape((-1, 28, 28))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9cad60c",
      "metadata": {
        "id": "d9cad60c"
      },
      "source": [
        "If your reshape worked correctly, the following code should show a apple, bird, or cookie sketch.  If it didn't work, it will look like random noise.  If you're not sure, you can always try more images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b88980bd",
      "metadata": {
        "id": "b88980bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "e736772a-98a8-4259-af50-8d066b844b81"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7d4ce2b087f0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(x_train[0]) # Should be a recognizable image if we reshaped correctly"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "604fdb13",
      "metadata": {
        "id": "604fdb13"
      },
      "source": [
        "As the last step before creating the neural network, we will turn the labels into one-hot encodings, like [0,0,1] instead of 2.  There's a handy keras function that does this, and we're just going to call it for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "862986ee",
      "metadata": {
        "id": "862986ee"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "num_classes = 3\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3d7790c",
      "metadata": {
        "id": "c3d7790c"
      },
      "source": [
        "(5, 9pts) Rather than create a network completely from scratch, it makes sense to start with a network that you know does something similar.  One of the keras tutorials shows how to do digit recognition with a relatively small deep neural network.  The dataset, the MNIST digit dataset, is very similar to ours because it consists of 28 x 28 black and white line drawings.  Consult the \"Build a Model\" and \"Train the model\" sections of https://keras.io/examples/vision/mnist_convnet/ and get that neural network running on our apple, bird, and cookie data.  (Be sure to leave a comment indicating that you borrowed the structure from there.)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "input_shape = (28,28,1)\n",
        "batch_size = 128\n",
        "epochs = 15"
      ],
      "metadata": {
        "id": "jMppq6PvCpab"
      },
      "id": "jMppq6PvCpab",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "07638a47",
      "metadata": {
        "id": "07638a47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86f4bba4-8500-4ab9-d611-9cde78d5608c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 13, 13, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 5, 5, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1600)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1600)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3)                 4803      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23619 (92.26 KB)\n",
            "Trainable params: 23619 (92.26 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# TODO borrow the MNIST model with attribution\n",
        "# model architecture from fchollet\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "044efa90",
      "metadata": {
        "id": "044efa90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8e4d377-44f3-4815-c3d5-ca0a39fb5ee3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "2161/2161 [==============================] - 24s 6ms/step - loss: 0.1320 - accuracy: 0.9589 - val_loss: 0.0872 - val_accuracy: 0.9719\n",
            "Epoch 2/15\n",
            "2161/2161 [==============================] - 11s 5ms/step - loss: 0.0852 - accuracy: 0.9728 - val_loss: 0.0703 - val_accuracy: 0.9773\n",
            "Epoch 3/15\n",
            "2161/2161 [==============================] - 12s 5ms/step - loss: 0.0738 - accuracy: 0.9764 - val_loss: 0.0641 - val_accuracy: 0.9789\n",
            "Epoch 4/15\n",
            "2161/2161 [==============================] - 12s 5ms/step - loss: 0.0687 - accuracy: 0.9777 - val_loss: 0.0601 - val_accuracy: 0.9797\n",
            "Epoch 5/15\n",
            "2161/2161 [==============================] - 13s 6ms/step - loss: 0.0652 - accuracy: 0.9789 - val_loss: 0.0591 - val_accuracy: 0.9803\n",
            "Epoch 6/15\n",
            "2161/2161 [==============================] - 12s 5ms/step - loss: 0.0619 - accuracy: 0.9798 - val_loss: 0.0569 - val_accuracy: 0.9813\n",
            "Epoch 7/15\n",
            "2161/2161 [==============================] - 13s 6ms/step - loss: 0.0600 - accuracy: 0.9801 - val_loss: 0.0557 - val_accuracy: 0.9819\n",
            "Epoch 8/15\n",
            "2161/2161 [==============================] - 11s 5ms/step - loss: 0.0586 - accuracy: 0.9807 - val_loss: 0.0539 - val_accuracy: 0.9824\n",
            "Epoch 9/15\n",
            "2161/2161 [==============================] - 11s 5ms/step - loss: 0.0572 - accuracy: 0.9814 - val_loss: 0.0522 - val_accuracy: 0.9826\n",
            "Epoch 10/15\n",
            "2161/2161 [==============================] - 12s 5ms/step - loss: 0.0562 - accuracy: 0.9814 - val_loss: 0.0549 - val_accuracy: 0.9819\n",
            "Epoch 11/15\n",
            "2161/2161 [==============================] - 12s 5ms/step - loss: 0.0547 - accuracy: 0.9816 - val_loss: 0.0514 - val_accuracy: 0.9826\n",
            "Epoch 12/15\n",
            "2161/2161 [==============================] - 12s 6ms/step - loss: 0.0539 - accuracy: 0.9821 - val_loss: 0.0510 - val_accuracy: 0.9830\n",
            "Epoch 13/15\n",
            "2161/2161 [==============================] - 12s 6ms/step - loss: 0.0529 - accuracy: 0.9823 - val_loss: 0.0528 - val_accuracy: 0.9821\n",
            "Epoch 14/15\n",
            "2161/2161 [==============================] - 12s 5ms/step - loss: 0.0524 - accuracy: 0.9825 - val_loss: 0.0500 - val_accuracy: 0.9829\n",
            "Epoch 15/15\n",
            "2161/2161 [==============================] - 12s 5ms/step - loss: 0.0522 - accuracy: 0.9824 - val_loss: 0.0499 - val_accuracy: 0.9831\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d4bd1cd9900>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# TODO compile and fit the model\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model.fit(x_train, y_train, batch_size = batch_size, epochs=epochs, validation_split=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1de32e51",
      "metadata": {
        "id": "1de32e51"
      },
      "source": [
        "(6, 2pts) Evaluate the model on the test set with the code below.  You should have a test accuracy well above 90%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "57781026",
      "metadata": {
        "id": "57781026",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "370032f0-773a-43fa-ff67-cee150c5db04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.047915734350681305\n",
            "Test accuracy: 0.9842401146888733\n"
          ]
        }
      ],
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce02d1d3",
      "metadata": {
        "id": "ce02d1d3"
      },
      "source": [
        "## Cats and Dogs\n",
        "\n",
        "It was relatively easy to get good performance on that task, because the size of each input is small and the features needed for success weren't too complicated.  We'll now try a classification task with real images.  This is a moderately well-known \"cats and dogs\" dataset.  Unzip the dogs-vs-cats dataset in the current directory.  You can examine the files to see pictures of cats and dogs with varying dimensions and varying poses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d_7Hbn3ug0YC",
      "metadata": {
        "id": "d_7Hbn3ug0YC"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OUdMf-mCg24l",
      "metadata": {
        "id": "OUdMf-mCg24l"
      },
      "outputs": [],
      "source": [
        "!unzip gdrive/MyDrive/NNAssign/dogs-vs-cats.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RUN0ls0qmat_",
      "metadata": {
        "id": "RUN0ls0qmat_"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6928900a",
      "metadata": {
        "id": "6928900a"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "image_size = (180, 180)\n",
        "batch_size = 32\n",
        "# Data from https://www.kaggle.com/competitions/dogs-vs-cats/\n",
        "# Code from https://keras.io/examples/vision/image_classification_from_scratch/\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"dogs-vs-cats/train\",\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=1337,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"dogs-vs-cats/train\",\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=1337,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f21331dd",
      "metadata": {
        "id": "f21331dd"
      },
      "source": [
        "(7, 9pts) We'll suppose that the closest starting point network we have on hand for this is the MNIST network again.  Adapt it to this dataset with the following changes:\n",
        "\n",
        "* The input shape is 180 x 180 x 3.\n",
        "* Put the rescaling of dividing values by 255 in the network itself with a layers.Rescaling() layer after the Input layer.  https://www.tensorflow.org/api_docs/python/tf/keras/layers/Rescaling\n",
        "* The final \"softmax\" activation function is only appropriate for multiclass classification.  Change this to a more appropriate activation function for binary classification.\n",
        "* Name the model model2 to work with the training code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f953ab4",
      "metadata": {
        "id": "2f953ab4"
      },
      "outputs": [],
      "source": [
        "# TODO create model2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef1ee68b",
      "metadata": {
        "id": "ef1ee68b"
      },
      "source": [
        "You can train the network using the following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4075fc4",
      "metadata": {
        "id": "d4075fc4"
      },
      "outputs": [],
      "source": [
        "epochs = 15\n",
        "\n",
        "model2.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "model2.fit(train_ds, epochs=epochs, validation_data=val_ds) # Validation data instead of fraction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09a9c2b3",
      "metadata": {
        "id": "09a9c2b3"
      },
      "source": [
        "(8, 6pts) The validation loss went down and then up, indicating overfitting, so we possibly could proceed to regularization steps at this point; nevertheless, for photos, we probably want to try some deeper architectures than for the hand drawings.  Probably, the features extracted for the MNIST and apple/bird/cookie datasets are simpler than what's necessary for photos.  This implies needing more layers.  One strategy used by some well-known networks is to add blocks each consisting of a Conv2D layer followed by MaxPooling2D layer, where each Conv2D layer has twice as many filters as the last one (so 32, 64, 128 ...).  You can see our current architecture already follows this pattern.  Try training networks with one, two, and three more of these blocks between the last MaxPooling2D layer and the Flatten() call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e26e5121",
      "metadata": {
        "id": "e26e5121"
      },
      "outputs": [],
      "source": [
        "# TODO define model3 with one more block of Conv2D and pooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffd6ee5f",
      "metadata": {
        "id": "ffd6ee5f"
      },
      "outputs": [],
      "source": [
        "epochs = 15\n",
        "\n",
        "model3.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "model3.fit(train_ds, epochs=epochs, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TlJfMTjHs_J_",
      "metadata": {
        "id": "TlJfMTjHs_J_"
      },
      "outputs": [],
      "source": [
        "# TODO define model4 with two more blocks of Conv2D and pooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J9zrSy54tC1g",
      "metadata": {
        "id": "J9zrSy54tC1g"
      },
      "outputs": [],
      "source": [
        "epochs = 15\n",
        "\n",
        "model4.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "model4.fit(train_ds, epochs=epochs, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RVRi9HamtUdg",
      "metadata": {
        "id": "RVRi9HamtUdg"
      },
      "outputs": [],
      "source": [
        "# TODO model5 with three more blocks of Conv2D and pooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QanXeQYSteAW",
      "metadata": {
        "id": "QanXeQYSteAW"
      },
      "outputs": [],
      "source": [
        "epochs = 15\n",
        "\n",
        "model5.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "model5.fit(train_ds, epochs=epochs, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hztQQ92WCPlR",
      "metadata": {
        "id": "hztQQ92WCPlR"
      },
      "source": [
        "(9, 6pts) Architecture search could continue all day, but let's take a different approach to improving the network further.  When the accuracy on the training data is higher than the validation accuracy, this is evidence of overfitting.  Two approaches to handling overfitting are:\n",
        "\n",
        "* More dropout.  We have one big layer of dropout, but we could have one or more additional layers of Dropout(0.2).\n",
        "* Data augmentation.  If the dataset is randomly rotated and flipped, this encourages features that are robust against these transformations, and it makes it harder to overfit the data.  We can work this into the pipeline itself with layers.RandomFlip(\"horizontal\") and layers.RandomRotation(0.1).\n",
        "\n",
        "Train a new network with these two ideas worked into your best architecture so far.  (Your best architecture is the one that you achieved the highest validation accuracy on.)  The dropout locations are up to you.  Use 20 epochs instead of 15, since these methods slow down the learning somewhat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "326045fe",
      "metadata": {
        "id": "326045fe"
      },
      "outputs": [],
      "source": [
        "# TODO model6 with data augmentation and some extra dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfe4a031",
      "metadata": {
        "id": "cfe4a031"
      },
      "outputs": [],
      "source": [
        "epochs = 20\n",
        "\n",
        "model6.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "model6.fit(train_ds, epochs=epochs, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sA1Ez4cRH3O2",
      "metadata": {
        "id": "sA1Ez4cRH3O2"
      },
      "source": [
        "(10, 7pts) Rather than building a model from nearly scratch, a different approach (\"transfer learning\") is to take an existing model that did something similar, including its trained weights, and retrain only a few layers at the very end of the model, keeping the rest of the model \"frozen.\"  This allows complex features learned from a lot of data to be used on smaller problems with less data, and it saves on training time as well.\n",
        "\n",
        "You can base your code on the transfer learning example provided in lecture.  (Note that since this is a binary classification problem instead of multiclass, you will need to change the final layer's activation function and the loss function.)  You should keep the \"adam\" optimizer we've been using all along.  You should only need to train to 7 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d461f2a",
      "metadata": {
        "id": "5d461f2a"
      },
      "outputs": [],
      "source": [
        "# TODO:  define model7 using transfer learning\n",
        "model7.fit(train_ds, epochs=7,validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efHZL1P6yyA5",
      "metadata": {
        "id": "efHZL1P6yyA5"
      },
      "source": [
        "(11, 19pts) Last, answer these questions.\n",
        "\n",
        "(a, 6 pts) Suppose we have a small perceptron with 3 inputs, no hidden units, and one output unit.  The 2 inputs represent car attributes:  cost (normalized to [0,1]), and mileage on the car (normalized to [0,1]).  Assume the input neurons have no activation functions.  Can this tiny network learn the rule, “fire if cost < 0.5, or mileage < 0.5”?  If so, specify the weights in the network.  If not, describe the smallest network you can that can effectively represent this rule (assuming the inputs still don’t have activation functions and the output neuron’s activation function is a step function) - give specific weights that would work."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GssQEsPyh4Nf",
      "metadata": {
        "id": "GssQEsPyh4Nf"
      },
      "source": [
        "**TODO**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "laXJ8j3qhlDa",
      "metadata": {
        "id": "laXJ8j3qhlDa"
      },
      "source": [
        "(b, 3pts) Give values for a 3x3 convolutional filter that would plausibly detect small X’s in the image - the meeting of two diagonal lines. Assume the lines are both white or both black (your choice)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SIUjgKExzl2q",
      "metadata": {
        "id": "SIUjgKExzl2q"
      },
      "source": [
        "**TODO**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zvUg7PaZ0PZY",
      "metadata": {
        "id": "zvUg7PaZ0PZY"
      },
      "source": [
        "(c, 3pts) Transfer learning is effective, and it's more effective the more similar the original task was to the present one.  Skim the Wikipedia page on ImageNet, the dataset used to train VGG-16 (and maybe follow up by searching this subset of ImageNet's classes: https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a).  Is there reason to think transfer learning with VGG-16 might be particularly effective for classifying cats and dogs?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Qu5ylggG3GaV",
      "metadata": {
        "id": "Qu5ylggG3GaV"
      },
      "source": [
        "**TODO**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7ffa227",
      "metadata": {
        "id": "f7ffa227"
      },
      "source": [
        "(d, 3pts) Suppose we introduce a new activation function in all the neurons in our neural network, g(x) = sin(x). We then notice vanishing gradient problems. What are some values of x that would be particularly problematic for this function in backpropagation?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d50a5d7e",
      "metadata": {
        "id": "d50a5d7e"
      },
      "source": [
        "**TODO**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78d9ac1d",
      "metadata": {
        "id": "78d9ac1d"
      },
      "source": [
        "(e, 4pts) A museum wants to train a neural network to identify plants, using museum plant specimens. In some cases they only have one example of a species, though, making a train/test split difficult.  They decide to use data augmentation in these cases, putting most of the augmented images in the training set with the original image, but also putting a few augmented images in the test set in each case.  Will this classifier have good performance \"in the wild\" on these species?  Why or why not?  Would we get better performance if we just trained on the unaugmented, single image?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfe23f34",
      "metadata": {
        "id": "bfe23f34"
      },
      "source": [
        "**TODO**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oH6oywqOc04-",
      "metadata": {
        "id": "oH6oywqOc04-"
      },
      "source": [
        "**When you're done, use \"File->Download .ipynb\" and upload your .ipynb file to Blackboard, along with a PDF version (File->Print->Save as PDF) of your assignment.**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}