{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion Question:\n",
        "\n",
        "Utilitarianism is the ethical stance that tries to maximize the total utility experienced by everyone in the world - the sum of everyone's utility functions. If we wanted a robot or AI to behave ethically, we could try to program utilitarianism into how it makes choices, so that it predicts the results of its different actions and performs the action that results in a world with the most utility for everyone. Identify a problem or practical difficulty with this approach - either a problem with utilitarianism in general or a problem with executing it effectively on a robot."
      ],
      "metadata": {
        "id": "kn3-PHfv1Baf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LN3JQOjqRwKc"
      },
      "source": [
        "# LSTMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kriSCXeHRwKr"
      },
      "source": [
        "This exercise is based on the example at: https://keras.io/examples/generative/lstm_character_level_text_generation/.  It also borrows some ideas from the code in *Learning Deep Learning* by Magnus Ekman, which appears in the lecture slides."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4FJUU6kRwLA"
      },
      "source": [
        "We're going to complete the missing parts of an LSTM that predicts the next characters in some text.  This is the same task as the LSTM in lecture, but we take a different approach in places.  Our demo uses Mary Shelley's Frankenstein (via Project Gutenberg, www.gutenberg.org) as a training corpus.  This is rather short, but has the advantage of not taking too long to train during section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_to-zeqjRwLF"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JfXFIR3JSH7v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "outputId": "d0797333-5e41-47c1-c2d6-6ff9b8727cde"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-08fabe7d-9bd7-4cfc-bba7-e20d52dfd3c3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-08fabe7d-9bd7-4cfc-bba7-e20d52dfd3c3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Skip this cell if not working in Google Colab\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload() # pick frankenstein_excerpt.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Npie78KpRwLL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3588b334-3dc0-4efc-dcd6-23328ff6160a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus length: 441033\n"
          ]
        }
      ],
      "source": [
        "with io.open(\"frankenstein.txt\", encoding=\"utf-8\") as f:\n",
        "    text = f.read().lower()\n",
        "text = text.replace(\"\\n\", \" \")  # We remove newlines chars for nicer display\n",
        "print(\"Corpus length:\", len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "r8cqgzeCiyOx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e37469d-5f93-47dc-8673-831318505889"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chars: 57\n"
          ]
        }
      ],
      "source": [
        "# Make lookup tables, character<->index\n",
        "chars = sorted(list(set(text)))\n",
        "print(\"Total chars:\", len(chars))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdXA4zEJitwq"
      },
      "source": [
        "You'll now write 2 functions to practice turning a time series into training data for an LSTM.\n",
        "\n",
        "**make_sequences(text, seqlen, step)**:  This function should return two lists.  The first list should contain subsequences of *text*.  These should each have *seqlen* characters that were contiguous in the original string.  Each subsequence should start *step* characters after the last.  So make_sequences('example!', 3, 2) should return ['exa','amp', 'ple'] as the first return value.  For each string in the first return value, the corresponding string in the second return value should be the character that comes next:  ['m', 'l', '!'] in the example.  (Stop iterating through the text when there aren't enough characters for a next character in the second list.)\n",
        "\n",
        "**to_one_hot(seqs, nexts, char_indices)**: Returns two matrices, X and y.  X is a 3D array where X[i,j,:] is a one-hot encoding of the jth character of sequence i (so 1 at the right character index and 0 elsewhere). y is a 2D array where y[i,:] is a one-hot encoding of nexts[i].  char_indices is assumed to be the dictionary of the same name created earlier.  (For efficiency, pass dtype=bool to your arrays upon creation.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "4PPwK6M2lolM"
      },
      "outputs": [],
      "source": [
        "def make_sequences(text, seqlen, step):\n",
        "  # TODO\n",
        "  sentences = []\n",
        "  next_chars = []\n",
        "  i = i\n",
        "  while i + seqlen < len(text):\n",
        "    sentences.append(text[i : i+seqlen])\n",
        "    next_chars.append(text[i + seqlen])\n",
        "    i += step\n",
        "\n",
        "  return sentences, next_chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "esrp9pOjnWbZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9682c8ac-8871-4d20-a9da-86d16b30965f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['exa', 'amp', 'ple'], ['m', 'l', '!'])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "make_sequences('example!', 3, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "FWfsVjRWnknw"
      },
      "outputs": [],
      "source": [
        "def to_one_hot(seqs, nexts, char_indices):\n",
        "  # TODO\n",
        "  X = np.zeros(shape=(len(seqs), len(seqs[0]), len(char_indices)), dtype=bool)\n",
        "  y = np.zeros(shape=(len(seqs), len(char_indices)), dtype=bool)\n",
        "  for i in range(len(seqs)):\n",
        "    for j in range(len(seqs[i])):\n",
        "      X[i][j][char_indices[seqs[i][j]]] = 1\n",
        "    y[i][char_indices[nexts[i]]] = 1\n",
        "  return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "0yXfB6N0oeBE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7b51cc4-e661-4b0c-db4a-f4a421f6d885"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[[False, False,  True, False, False, False, False],\n",
              "         [False, False, False, False, False, False,  True],\n",
              "         [False,  True, False, False, False, False, False]],\n",
              " \n",
              "        [[False,  True, False, False, False, False, False],\n",
              "         [False, False, False, False,  True, False, False],\n",
              "         [False, False, False, False, False,  True, False]],\n",
              " \n",
              "        [[False, False, False, False, False,  True, False],\n",
              "         [False, False, False,  True, False, False, False],\n",
              "         [False, False,  True, False, False, False, False]]]),\n",
              " array([[False, False, False, False,  True, False, False],\n",
              "        [False, False, False,  True, False, False, False],\n",
              "        [ True, False, False, False, False, False, False]]))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# Create a tiny dict for testing purposes\n",
        "test_chars = sorted(list(set('example!')))\n",
        "test_char_indices = dict((c, i) for i, c in enumerate(test_chars))\n",
        "seqs, nexts = make_sequences('example!', 3, 2)\n",
        "to_one_hot(seqs, nexts, test_char_indices)\n",
        "# Examine the one-hot encodings ... do they make sense for this example?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUSp5pVErdMi"
      },
      "source": [
        "Once you're satisfied with your functions, you can proceed to create the training data from the Frankenstein text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "y2XKF0u7rPS1"
      },
      "outputs": [],
      "source": [
        "seqlen = 40\n",
        "step = 3\n",
        "seqs, nexts = make_sequences(text, seqlen, step)\n",
        "X, y = to_one_hot(seqs, nexts, char_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07Ue0mQqr5H_"
      },
      "source": [
        "Once the data is in the right format, there's not much to creating a basic LSTM that can train from it and make predictions.  We've omitted just the last layer, the output layer, from the LSTM-based neural network below.  Can you figure out what it should be?  Hint:  the output is a choice of letter, again in the one-hot encoding set up earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "WrIbgmS0RwLM"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(seqlen, len(chars))),\n",
        "        layers.LSTM(128),\n",
        "        # TODO:  layers.Dense(last_layer_size???, activation=???),\n",
        "        layers.Dense(len(char_indices), activation='softmax')\n",
        "    ]\n",
        ")\n",
        "optimizer = keras.optimizers.RMSprop(learning_rate=0.01)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUQuvZBzstR_"
      },
      "source": [
        "Before we train the model, we'll delve a little bit into generative models, since the model will generate sample predicted sequences with each epoch.\n",
        "\n",
        "Lecture covered sequence generation with an LSTM using beam search, which creates several possible continuations of the text and chooses the most likely overall.  A different approach is to sample each next letter randomly, as a function of the activation strength for that character.  *How* randomly is a matter of taste and varies from application to application - sometimes you want a generative model to be a little surprising, and sometimes you want it to be as unsurprising as possible.\n",
        "\n",
        "Take a look at the code for sample(), below.  It implements a common formula for the probability of sampling from a multinomial distribution in a way partly governed by a temperature parameter, T.  What is the formula for the probability of character i, as a function of preds(i) and temperature T?  (Try to simplify if you can.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Xy_IzIalRwLN"
      },
      "outputs": [],
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype(\"float64\")\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgob9x8xwLwt"
      },
      "source": [
        "**The probability of being picked is $TODO$**\n",
        "\n",
        "The probability of being picked is $e^{\\frac{log(preds)}{temperature}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8opHK6PyFLz"
      },
      "source": [
        "Try the sample() function on the array [0.25, 0.25, 0.5] for very large T (100) and very small T (0.1).  What do high and low temperature do?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "H_9vWvaAysv-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44b0ee2b-88e9-4866-9ad2-66888e218511"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "# Try rerunning this cell\n",
        "sample_array = [0.25, 0.25, 0.5]\n",
        "print(sample(sample_array, 100))\n",
        "print(sample(sample_array, 0.1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpIWiTH5zAbp"
      },
      "source": [
        "**TODO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NssAvvfuzPHY"
      },
      "source": [
        "Now go ahead and train the LSTM.  You may want to do this in Google Colab with GPU acceleration (Edit->Notebook settings), unless you have a fast GPU of your own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "XZdP9lwPRwLb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdab5f4f-3711-4c6f-9d71-88923f6654d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1149/1149 [==============================] - 12s 6ms/step - loss: 2.0128\n",
            "\n",
            "Generating text after epoch: 0\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \"th almost no restrictions whatsoever.  y\"\n",
            "...Generated:  ou are the seemet of her ood the accure of the our\n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \"th almost no restrictions whatsoever.  y\"\n",
            "...Generated:  e une i was ecelizingeng-urhoned beadayd a \"und sh\n",
            "\n",
            "1149/1149 [==============================] - 7s 6ms/step - loss: 1.6205\n",
            "\n",
            "Generating text after epoch: 1\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \"hen, but i have endured misery which not\"\n",
            "...Generated:  hed, and the elizabeth of the ever have many strus\n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \"hen, but i have endured misery which not\"\n",
            "...Generated:   angathion; gutixy om surpolding this truen; that \n",
            "\n",
            "1149/1149 [==============================] - 6s 6ms/step - loss: 1.4909\n",
            "\n",
            "Generating text after epoch: 2\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \"hy and compassion; he drew a chair close\"\n",
            "...Generated:   sected a more, the creatures of his creature have\n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \"hy and compassion; he drew a chair close\"\n",
            "...Generated:   ond ulminted mesited the brin have voyable anguin\n",
            "\n",
            "1149/1149 [==============================] - 6s 6ms/step - loss: 1.4244\n",
            "\n",
            "Generating text after epoch: 3\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \"mpanion must be of the same species and \"\n",
            "...Generated:  spirit of the pressed and spot and in the procersi\n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \"mpanion must be of the same species and \"\n",
            "...Generated:  being, but a transhing to comploled to the mountai\n",
            "\n",
            "1149/1149 [==============================] - 7s 6ms/step - loss: 1.3809\n",
            "\n",
            "Generating text after epoch: 4\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \" determined to quit my island at the exp\"\n",
            "...Generated:  ressed my procerse of the stention of the expressi\n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \" determined to quit my island at the exp\"\n",
            "...Generated:  ectic of even wrance and are at teclurion.  the gr\n",
            "\n",
            "1149/1149 [==============================] - 7s 6ms/step - loss: 1.3499\n",
            "\n",
            "Generating text after epoch: 5\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \"n food and shelter; at length i perceive\"\n",
            "...Generated:  d to remained in my talemary hands are to speek me\n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \"n food and shelter; at length i perceive\"\n",
            "...Generated:  d the heart when i was joy on me, when a look; and\n",
            "\n",
            "1149/1149 [==============================] - 6s 6ms/step - loss: 1.3256\n",
            "\n",
            "Generating text after epoch: 6\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \"e sweet and mournful than i had ever hea\"\n",
            "...Generated:  rd them when the thousand heard my streed the comp\n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \"e sweet and mournful than i had ever hea\"\n",
            "...Generated:  rte hasteled years. he evidted, ik, and the projem\n",
            "\n",
            "1149/1149 [==============================] - 7s 6ms/step - loss: 1.3060\n",
            "\n",
            "Generating text after epoch: 7\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \".  was there no injustice in this? am i \"\n",
            "...Generated:  had gone me with the employ roture was sought the \n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \".  was there no injustice in this? am i \"\n",
            "...Generated:  lay but   lond beheld betriphings aqvar.  one of s\n",
            "\n",
            "1149/1149 [==============================] - 6s 6ms/step - loss: 1.2901\n",
            "\n",
            "Generating text after epoch: 8\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \"iance for any particular state visit htt\"\n",
            "...Generated:   for the stake of my cottage to death and i should\n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \"iance for any particular state visit htt\"\n",
            "...Generated:  , and i read, to    freadful, but i was absent to \n",
            "\n",
            "1149/1149 [==============================] - 8s 7ms/step - loss: 1.2774\n",
            "\n",
            "Generating text after epoch: 9\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \" any interruption in your correspondence\"\n",
            "...Generated:   and appeared to the former for the copyright of h\n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \" any interruption in your correspondence\"\n",
            "...Generated:   you as man, have leated?  and i her barrifatid as\n",
            "\n",
            "1149/1149 [==============================] - 6s 5ms/step - loss: 1.2665\n",
            "\n",
            "Generating text after epoch: 10\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \"sentations concerning the copyright stat\"\n",
            "...Generated:  es of the courtly.  the same was a strange and the\n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \"sentations concerning the copyright stat\"\n",
            "...Generated:  e of his commone.  the cent of owe i durlpplsting \n",
            "\n",
            "1149/1149 [==============================] - 6s 6ms/step - loss: 1.2565\n",
            "\n",
            "Generating text after epoch: 11\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \"stened to the exhortations of her father\"\n",
            "...Generated:   and a mountain the spreage of the only saw and th\n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \"stened to the exhortations of her father\"\n",
            "...Generated:   madice.  you 5en of confluence.  \"  he excititned\n",
            "\n",
            "1149/1149 [==============================] - 6s 5ms/step - loss: 1.2460\n",
            "\n",
            "Generating text after epoch: 12\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \" of another would destroy the solitary g\"\n",
            "...Generated:  urge of a feeling of the project gutenberg-tm ente\n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \" of another would destroy the solitary g\"\n",
            "...Generated:  urenly to us there attended chouse from a vistical\n",
            "\n",
            "1149/1149 [==============================] - 8s 7ms/step - loss: 1.2387\n",
            "\n",
            "Generating text after epoch: 13\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \"y adopted, for there is a great differen\"\n",
            "...Generated:  t and state and the project gutenberg and and ment\n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \"y adopted, for there is a great differen\"\n",
            "...Generated:  ts, were reamanties that passual]y'sunder to searn\n",
            "\n",
            "1149/1149 [==============================] - 6s 5ms/step - loss: 1.2307\n",
            "\n",
            "Generating text after epoch: 14\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \"he person you received the work from.  i\"\n",
            "...Generated:   considered on the improvements of a deep of the u\n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \"he person you received the work from.  i\"\n",
            "...Generated:   die, and a ses.  can it wasquered the been lookin\n",
            "\n",
            "1149/1149 [==============================] - 6s 5ms/step - loss: 1.2240\n",
            "\n",
            "Generating text after epoch: 15\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \"en me far from the coast from which i ha\"\n",
            "...Generated:  d not since the strange of the strangers and mine \n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \"en me far from the coast from which i ha\"\n",
            "...Generated:  d not chand the langele hanfiken, has berrected am\n",
            "\n",
            "1149/1149 [==============================] - 6s 5ms/step - loss: 1.2169\n",
            "\n",
            "Generating text after epoch: 16\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \"is companion.  in the mean time i worked\"\n",
            "...Generated:   passed has a finish that the stranger of decayion\n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \"is companion.  in the mean time i worked\"\n",
            "...Generated:   to seed, also be newers of my town, possibul with\n",
            "\n",
            "1149/1149 [==============================] - 6s 5ms/step - loss: 1.2131\n",
            "\n",
            "Generating text after epoch: 17\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \"econdary object; i was principally occup\"\n",
            "...Generated:  l to her fear, the gentle minute of my traves of m\n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \"econdary object; i was principally occup\"\n",
            "...Generated:  ied seet her thauld were fit the happy, i pacaled \n",
            "\n",
            "1149/1149 [==============================] - 7s 6ms/step - loss: 1.2081\n",
            "\n",
            "Generating text after epoch: 18\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \"heart was poisoned with remorse.  think \"\n",
            "...Generated:  in the most more sun and mist went to the project \n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \"heart was poisoned with remorse.  think \"\n",
            "...Generated:  ulter, he had enduem; while, the winds of a dunk h\n",
            "\n",
            "1149/1149 [==============================] - 7s 6ms/step - loss: 1.2033\n",
            "\n",
            "Generating text after epoch: 19\n",
            "...Temperature: 0.5\n",
            "...Generating with seed: \"tion i traversed the northern highlands \"\n",
            "...Generated:  and the monster will endeavour to the more which i\n",
            "\n",
            "...Temperature: 1.0\n",
            "...Generating with seed: \"tion i traversed the northern highlands \"\n",
            "...Generated:  effect me than i had after her flew that i had tak\n",
            "\n"
          ]
        }
      ],
      "source": [
        "epochs = 20\n",
        "batch_size = 128\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.fit(X, y, batch_size=batch_size, epochs=1)\n",
        "    print()\n",
        "    print(\"Generating text after epoch: %d\" % epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(text) - seqlen - 1)\n",
        "    for temp in [0.5, 1.0]:\n",
        "        print(\"...Temperature:\", temp)\n",
        "\n",
        "        generated = \"\"\n",
        "        sentence = text[start_index : start_index + seqlen]\n",
        "        print('...Generating with seed: \"' + sentence + '\"')\n",
        "\n",
        "        for i in range(50):\n",
        "            x_pred = np.zeros((1, seqlen, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char_indices[char]] = 1.0\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, temp)\n",
        "            next_char = indices_char[next_index]\n",
        "            sentence = sentence[1:] + next_char\n",
        "            generated += next_char\n",
        "\n",
        "        print(\"...Generated: \", generated)\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xN0eHBPRhEcB"
      },
      "source": [
        "You might still be getting some nonsense at epoch 20, although it should be noticeably better than at the start.  This setup for training was chosen with speed of training as the foremost concern, since LSTMs can take a long time to train.  You can provide a longer training corpus - there are rather longer books available in plain text at Project Gutenberg. You can run for more epochs - Chollet, the original author of this example, says 20 is a bare minimum and 40 is recommended.  Or you can try augmenting the LSTM architecture with another layer; there's an example of this in the lecture slides.  (Be sure to set return_sequences to True for the lower LSTM layer.)\n",
        "\n",
        "Try one of these approaches, and compare your results with a neighbor who chose a different one."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}