{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4yvI-kf-myN"
   },
   "source": [
    "# MDPs and Q-learning On \"Ice\" (60 points possible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-iEuDHFs-ixs"
   },
   "source": [
    "In this assignment, we’ll revisit Markov Decision Processes while also trying out Q-Learning, the reinforcement learning approach that associates utilities with attempting actions in states.\n",
    "The problem that we’re attempting to solve is the following:\n",
    "\n",
    "1.  There is a grid of spaces in a rectangle.  Each space can contain a pit (negative reward), gold (positive reward), or nothing.\n",
    "2.  The rectangle is effectively surrounded by walls, so anything that would move you outside the rectangle, instead moves you to the edge of the rectangle.\n",
    "3.  The floor is icy.  Any attempt to move in a cardinal direction results in moving a somewhat random number of spaces in that direction.  The exact probabilities of moving each number of spaces are given in the problem description.  (If you slide too far, see rule #2.)\n",
    "4.  Landing on a pit or gold effectively “ends the run,” for both a Q learner and an agent later trying out the policy.  It’s game over.  (To simulate this during Q learning, set all Q values for the space to equal its reward, then start over from a random space.)  Note that it's still possible to slide past a pit or gold - this doesn't end the run.\n",
    "\n",
    "A sample input looks like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "L_PSiQiO-_2y"
   },
   "outputs": [],
   "source": [
    "sampleMDP = \"\"\"0.7 0.2 0.1\n",
    "- - P - -\n",
    "- - G P -\n",
    "- - P - -\n",
    "- - - - -\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CmLASMB_Gsd"
   },
   "source": [
    "\n",
    "The first line says that the probabilities of moving one, two, or three spaces in the direction of movement are 0.7, 0.2, and 0.1.   The rest is a map of the environment, where a dash is an empty space, P is a pit, and G is gold.\n",
    "\n",
    "Your job is to finish the code below for mdp_solve() and q_solve().  These take a problem description like the one pictured above, and return a policy giving the recommended action to take in each empty square (U=up, R=right, D=down, L=left).\n",
    "\n",
    "**1, 17 points)**  mdp_solve() should use value iteration and the Bellman equation.  ITERATIONS will refer to the number of complete passes you perform over all states.  You can initialize the utilities to the rewards of each state.  Don’t update the rewards spaces from their initial rewards; since they end the trial, they have no future utility.  Don't update utilities in-place as you iterate through them, but create a fresh array of utilities with each pass, in order to avoid biasing moves in the directions that have already been updated.\n",
    "\n",
    "**2, 24 points)**  q_solve() will run ITERATIONS trials in which a learner starts in a random empty square and moves until it hits a pit or gold, in which case, the trial is over.  (If it was randomly dropped into gold or a pit, the trial is immediately over.)  The learner moves by deciding randomly whether to choose a random direction (with probability EXPLORE_PROB) or move according to the best Q-value of its current square (otherwise).  Simulate the results of the move on slippery ice to determine where the learner ended up - then apply the Q-learning equation given in lecture and the textbook.  (There are multiple Q-learning variants out there, so try to use the equations and practices described in lecture instead of using other sources, to avoid confusion.)\n",
    "\n",
    "The fact that a trial ends immediately on finding gold or a pit means that we want to handle those spaces in a special way.  Normally Q values are updated on moving to the next state, but we won’t see any next state in these cases.  So, to handle this, when the agent discovers one of these rewards, set all the Q values for that space to the associated reward before quitting the trial.  So, for example, if gold is worth 100 and it’s discovered in square x, Q(x,UP) = 100, Q(x,RIGHT) = 100, Q(x, DOWN) = 100, and Q(x, LEFT) = 100.  There’s no need to apply the rest of the Q update equation when the trial is ending, because that’s all about future rewards, and there’s no future when the trial is ending.  But now the spaces that can reach that space will evaluate themselves appropriately.  (Before being \"discovered,\" the square should have no utility.)\n",
    "\n",
    "You should use the GOLD_REWARD, PIT_REWARD, LEARNING_RATE, and DISCOUNT_FACTOR constants at the top of the code box below.\n",
    "\n",
    "Q-learning involves a lot of randomness and some arbitrary decisions when breaking ties, so two implementations can both be correct but recommend slightly different policies in the end, even if they have the same starting random seed.  While we provide some helpful premade maps below, your main guide for debugging will be common sense in deciding whether the policy created by your agent makes sense -- ie, agents following the policy will get gold without taking unnecessary risks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ZW7aHFXpUQ9l"
   },
   "outputs": [],
   "source": [
    "\"\"\" \"MDPs on Ice - Assignment 5\"\"\"\n",
    "\n",
    "import random\n",
    "import copy\n",
    "\n",
    "GOLD_REWARD = 250.0\n",
    "PIT_REWARD = -150.0\n",
    "DISCOUNT_FACTOR = 0.8\n",
    "EXPLORE_PROB = 0.2 # for Q-learning\n",
    "LEARNING_RATE = 0.01\n",
    "ITERATIONS = 20000\n",
    "MAX_MOVES = 1000\n",
    "ACTIONS = 4\n",
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3\n",
    "MOVES = ['U', 'R', 'D', 'L']\n",
    "\n",
    "# Fixed random number generator seed for result reproducibility --\n",
    "# don't use a random number generator besides this to match sol\n",
    "random.seed(340)\n",
    "\n",
    "class Problem:\n",
    "    \"\"\"Represents the physical space, transition probabilities, reward locations, and approach\n",
    "\n",
    "    ...in short, the info in the problem string\n",
    "\n",
    "    Attributes:\n",
    "        move_probs (List[float]):  probabilities of going 1,2,3 spaces\n",
    "        map (List[List(string)]]:  \"-\" (safe, empty space), \"G\" (gold), \"P\" (pit)\n",
    "\n",
    "    String format consumed looks like\n",
    "    0.7 0.2 0.1   [probability of going 1, 2, 3 spaces]\n",
    "    - - - - - - P - - - -   [space-delimited map rows]\n",
    "    - - G - - - - - P - -   [G is gold, P is pit]\n",
    "\n",
    "    You can assume the maps are rectangular, although this isn't enforced\n",
    "    by this constructor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, probstring):\n",
    "        \"\"\" Consume string formatted as above\"\"\"\n",
    "        self.map = []\n",
    "        for i, line in enumerate(probstring.splitlines()):\n",
    "            if i == 0:\n",
    "                self.move_probs = [float(s) for s in line.split()]\n",
    "            else:\n",
    "                self.map.append(line.split())\n",
    "\n",
    "    def solve(self, iterations, use_q):\n",
    "        \"\"\" Wrapper for MDP and Q solvers.\n",
    "\n",
    "        Args:\n",
    "            iterations (int):  Number of iterations (but these work differently for the two solvers)\n",
    "            use_q (bool):  False means use MDP value iteration, true means use Q-learning\n",
    "        Returns:\n",
    "            A Policy, in either case (what to do in each square; see class below)\n",
    "        \"\"\"\n",
    "\n",
    "        if use_q:\n",
    "            return q_solve(self, iterations)\n",
    "        return mdp_solve(self, iterations)\n",
    "\n",
    "class Policy:\n",
    "    \"\"\" Abstraction on the best action to perform in each state.\n",
    "\n",
    "    This is a string list-of-lists map similar to the problem input, but a character gives the best\n",
    "    action to take in each non-reward square (see MOVES constant at top of file).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, problem):\n",
    "        \"\"\"Args:\n",
    "\n",
    "        problem (Problem):  The MDP problem this is a policy for\n",
    "        \"\"\"\n",
    "        self.best_actions = copy.deepcopy(problem.map)\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"Join the characters in the policy into one big space-separated, multline string\"\"\"\n",
    "        return '\\n{}\\n'.format('\\n'.join([' '.join(row) for row in self.best_actions]))\n",
    "\n",
    "def roll_steps(move_probs, row, col, move, rows, cols):\n",
    "    \"\"\"Calculates the new coordinates that result from a move.\n",
    "\n",
    "    Includes the \"roll of the dice\" for transition probabilities and checking arena boundaries.\n",
    "\n",
    "    Helper for try_policy and q_solve - probably useful in your Q-learning implementation.\n",
    "\n",
    "    Args:\n",
    "        move_probs (List[float]):  Transition probabilities for the ice (from problem)\n",
    "        row, col (int, int):  location of agent before moving\n",
    "        move (string):  The direction of move as a MOVES character (not an int constant!)\n",
    "        rows, cols (int, int):  number of rows and columns in the map\n",
    "\n",
    "    Returns:\n",
    "        new_row, new_col (int, int):  The new row and column after moving\n",
    "    \"\"\"\n",
    "    displacement = 1\n",
    "    total_prob = 0\n",
    "    move_sample = random.random()\n",
    "    for p, prob in enumerate(move_probs):\n",
    "        total_prob += prob\n",
    "        if move_sample <= total_prob:\n",
    "            displacement = p+1\n",
    "            break\n",
    "    # Handle \"slipping\" into edge of map\n",
    "    new_row = row\n",
    "    new_col = col\n",
    "    if not isinstance(move, str):\n",
    "        print(\"Warning: roll_steps wants str for move, got a different type\")\n",
    "    if move == \"U\":\n",
    "        new_row -= displacement\n",
    "        if new_row < 0:\n",
    "            new_row = 0\n",
    "    elif move == \"R\":\n",
    "        new_col += displacement\n",
    "        if new_col >= cols:\n",
    "            new_col = cols-1\n",
    "    elif move == \"D\":\n",
    "        new_row += displacement\n",
    "        if new_row >= rows:\n",
    "            new_row = rows-1\n",
    "    elif move == \"L\":\n",
    "        new_col -= displacement\n",
    "        if new_col < 0:\n",
    "            new_col = 0\n",
    "    return new_row, new_col\n",
    "\n",
    "\n",
    "def try_policy(policy, problem, iterations):\n",
    "    \"\"\"Returns average utility per move of the policy.\n",
    "\n",
    "    Average utility is as measured by \"iterations\" random drops of an agent onto empty\n",
    "    spaces, running until gold, pit, or time limit MAX_MOVES is reached.\n",
    "\n",
    "    Doesn't necessarily play a role in your code, but you can try policies this\n",
    "    way\n",
    "\n",
    "    Args:\n",
    "        policy (Policy):  the policy the agent is following\n",
    "        problem (Problem):  the environment description\n",
    "        iterations (int):  the number of random trials to run\n",
    "    \"\"\"\n",
    "    total_utility = 0\n",
    "    total_moves = 0\n",
    "    for _ in range(iterations):\n",
    "        # Resample until we have an empty starting square\n",
    "        while True:\n",
    "            row = random.randrange(0, len(problem.map))\n",
    "            col = random.randrange(0, len(problem.map[0]))\n",
    "            if problem.map[row][col] == \"-\":\n",
    "                break\n",
    "        for moves in range(MAX_MOVES):\n",
    "            total_moves += 1\n",
    "            policy_rec = policy.best_actions[row][col]\n",
    "            # Take the move - roll to see how far we go, bump into map edges as necessary\n",
    "            row, col = roll_steps(problem.move_probs, row, col, policy_rec, \\\n",
    "                                  len(problem.map), len(problem.map[0]))\n",
    "            if problem.map[row][col] == \"G\":\n",
    "                total_utility += GOLD_REWARD\n",
    "                break\n",
    "            if problem.map[row][col] == \"P\":\n",
    "                total_utility += PIT_REWARD\n",
    "                break\n",
    "    return total_utility / total_moves\n",
    "\n",
    "def space_utility(space):\n",
    "    utility = 0\n",
    "    if space == 'P':\n",
    "        utility = PIT_REWARD\n",
    "    elif space == 'G':\n",
    "        utility = GOLD_REWARD\n",
    "    return utility\n",
    "\n",
    "# return the space within the bounds of the board (new_row, new_col) as coords\n",
    "def get_space(problem, row, col, row_change, col_change, dist):\n",
    "    return min(max(0, row + (row_change * dist)), len(problem.map) - 1), min(max(0, col + (col_change * dist)), len(problem.map[row]) - 1)\n",
    "\n",
    "def mdp_solve(problem, iterations):\n",
    "    \"\"\" Perform value iteration for the given number of iterations on the MDP problem.\n",
    "\n",
    "    Here, the squares with rewards can be initialized to the reward values, since value iteration\n",
    "    assumes complete knowledge of the environment and its rewards.\n",
    "\n",
    "    Args:\n",
    "        problem (Problem):  description of the environment\n",
    "        iterations (int):  number of complete passes over the utilities\n",
    "    Returns:\n",
    "        a Policy (though you may design this to return utilities as a second return value)\n",
    "    \"\"\"\n",
    "    # TODO calculate the policy\n",
    "    policy = Policy(problem=problem)\n",
    "    # initialize utility map\n",
    "    utility_map = []\n",
    "    # print(len(utility_map))\n",
    "    for i, row in enumerate(problem.map):\n",
    "        for j, space in enumerate(row):\n",
    "            if(j == 0):\n",
    "                utility_map.append([])\n",
    "            utility_map[i].append(space_utility(space))\n",
    "    \n",
    "    # print(problem.map)\n",
    "    # print(utility_map)\n",
    "    \n",
    "    for iter in range(iterations):\n",
    "        new_utility_map = []\n",
    "        # run the bellman equation to update the value property for each space\n",
    "        for i in range(len(utility_map)):\n",
    "            # print(\"i: \" + str(i))\n",
    "            # print(utility_map)\n",
    "            # print(utility_map[i])\n",
    "            for j in range(len(utility_map[i])):\n",
    "                if(j == 0):\n",
    "                    new_utility_map.append([])\n",
    "                max_util = 0\n",
    "                for move in MOVES:\n",
    "                    # changes for 'U'\n",
    "                    row_change = -1\n",
    "                    col_change = 0\n",
    "                    if move == 'R':\n",
    "                        row_change = 0\n",
    "                        col_change = 1\n",
    "                    elif move == 'D':\n",
    "                        row_change = 1\n",
    "                    elif move == 'L':\n",
    "                        row_change = 0\n",
    "                        col_change = -1\n",
    "                    \n",
    "                    # calculate the bellman equation sum for each distance that the player can move\n",
    "                    sum = 0\n",
    "                    for k,prob in enumerate(problem.move_probs):\n",
    "                        new_row, new_col = get_space(problem, i, j, row_change, col_change, k + 1)\n",
    "                        # new_space = problem.map[new_row][new_col]\n",
    "                        sum += prob*(DISCOUNT_FACTOR * utility_map[new_row][new_col] + space_utility(problem.map[i][j]))\n",
    "                    if sum > max_util:\n",
    "                        policy.best_actions[i][j] = move\n",
    "                        max_util = sum\n",
    "                new_utility_map[i].append(max_util)\n",
    "        utility_map = new_utility_map\n",
    "\n",
    "    \n",
    "    return policy, utility_map\n",
    "\n",
    "def q_solve(problem, iterations):\n",
    "    \"\"\"q_solve:  Use Q-learning to find a good policy on an MDP problem.\n",
    "\n",
    "    Each iteration corresponds to a random drop of the agent onto the map, followed by moving\n",
    "    the agent until a reward is reached or MAX_MOVES moves have been made.  When an agent\n",
    "    is sitting on a reward, update the utility of each move from the space to the reward value\n",
    "    and end the iteration.  (For simplicity, the agent also does this if just dropped there.)\n",
    "    The agent does not \"know\" reward locations in its Q-values before encountering the space\n",
    "    and \"discovering\" the reward.\n",
    "\n",
    "    Note that texts differ on when to pay attention to this reward - this code follows the\n",
    "    convention of scoring rewards of the space you are moving *from*, plus discounted best q-value\n",
    "    of where you landed.\n",
    "\n",
    "    Assume epsilon-greedy exploration.  Leave reward letters as-is in the policy,\n",
    "    to make it more readable.\n",
    "\n",
    "    Args:\n",
    "        problem (Problem):  The environment\n",
    "        iterations (int):  The number of runs from random start to reward encounter\n",
    "    Returns:\n",
    "        A Policy for the map\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    policy = Policy(problem=problem)\n",
    "    rows = len(problem.map)\n",
    "    cols = len(problem.map[0])\n",
    "    # store the Q(a, s) for each square shape = (row, col, 4)\n",
    "    Q = np.zeros(shape=(rows, cols, len(MOVES)))\n",
    "    # initialize Q with 0's\n",
    "    # for i in range(rows):\n",
    "    #     Q.append([])\n",
    "    #     for j in range(cols):\n",
    "    #         Q[i].append([])\n",
    "    #         for k in range(len(MOVES)):\n",
    "    #             Q[i][j].append(0)\n",
    "    \n",
    "    for iter in range(iterations):\n",
    "        # for each iteration, run an iteration of q learning\n",
    "        # drop the agent onto a random space on the map\n",
    "        row = random.randint(0, rows - 1)\n",
    "        col = random.randint(0, cols - 1)\n",
    "        for i in range(MAX_MOVES):\n",
    "            # check if the current space is a PIT or GOLD\n",
    "            # insert code here\n",
    "            if problem.map[row][col] == 'G' or problem.map[row][col] == 'P':\n",
    "                for j in range(len(MOVES)):\n",
    "                    Q[row][col][j] = space_utility(problem.map[row][col])\n",
    "                break\n",
    "            # until it reaches MAX_MOVES (or lands on gold or pit) keep moving it\n",
    "            if random.random() <= EXPLORE_PROB:\n",
    "                # pick a random direction to explore\n",
    "                move = random.randint(0, 3)\n",
    "                # get the new space given the chosen direction\n",
    "                new_row, new_col = roll_steps(move_probs=problem.move_probs, row=row, col=col, move=MOVES[move], rows=rows, cols=cols)\n",
    "                Q[row][col][move] += LEARNING_RATE*(space_utility(problem.map[row][col]) + DISCOUNT_FACTOR*max(Q[new_row][new_col]) - Q[row][col][move])\n",
    "                row = new_row\n",
    "                col = new_col\n",
    "            else:\n",
    "                # pick the best action for the given space\n",
    "                move = np.argmax(Q[row][col])\n",
    "                new_row, new_col = roll_steps(problem.move_probs, row, col, MOVES[move], rows, cols)\n",
    "                Q[row][col][move] += LEARNING_RATE*(space_utility(problem.map[row][col]) + DISCOUNT_FACTOR*max(Q[new_row][new_col]) - Q[row][col][move])\n",
    "                row = new_row\n",
    "                col = new_col\n",
    "    # print(Q)\n",
    "    \n",
    "    # populate the best moves for each position\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            policy.best_actions[i][j] = MOVES[np.argmax(Q[i][j])]\n",
    "    \n",
    "    return policy\n",
    "\n",
    "def new_q(rewards, utilities, r, c, new_r, new_c, movenum):\n",
    "    \"\"\" Q-learning function.  Returns the new Q-value for space (r,c).\n",
    "    It's recommended you code and test this before doing the overall Q-learning.\n",
    "\n",
    "    Should use the LEARNING_RATE and DISCOUNT_FACTOR.\n",
    "\n",
    "    Args:\n",
    "        rewards (List[List[float]]):  Reward amounts built into the problem map (indexed [r][c])\n",
    "        utilities (List[List[List[float]]]):  The Q-values for each action from each space.\n",
    "                                              (Indexed as [row][col][move])\n",
    "        r, c (int, int):  Row and column of our location before move\n",
    "        new_r, new_c (int, int):  Row and column of our location after move\n",
    "        movenum (int):  Integer index into the Q-values, corresponding to constants UP etc\n",
    "    Returns:\n",
    "        float - the new Q-value for the space we moved from\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return new_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5cuaEcLvAoYK"
   },
   "outputs": [],
   "source": [
    "deterministic_test = \"\"\"1.0\n",
    "- - P - -\n",
    "- - G P -\n",
    "- - P - -\n",
    "- - - - -\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3ABkxRiTA4Wi"
   },
   "outputs": [],
   "source": [
    "# Notice that we counterintuitively are most likely to go 2 spaces here\n",
    "very_slippy_test = \"\"\"0.2 0.7 0.1\n",
    "- - P - -\n",
    "- - G P -\n",
    "- - P - -\n",
    "- - - - -\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Lqg8ZZUCBYYl"
   },
   "outputs": [],
   "source": [
    "big_test = \"\"\"0.6 0.3 0.1\n",
    "- P - G - P - - G -\n",
    "P G - P - - - P - -\n",
    "P P - P P - P - P -\n",
    "P - - P P - - - - P\n",
    "- - - - - - - - P G\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "sHZ99I9uBmiH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R D D D D\n",
      "R R L L L\n",
      "U U U U U\n",
      "U U U U U\n",
      "\n",
      "[[355.5555555555556, 444.44444444444446, 405.55555555555554, 324.44444444444446, 259.5555555555556], [444.44444444444446, 555.5555555555555, 694.4444444444445, 405.55555555555554, 324.44444444444446], [355.5555555555556, 444.44444444444446, 405.55555555555554, 324.44444444444446, 259.5555555555556], [284.4444444444445, 355.5555555555556, 324.44444444444446, 259.5555555555556, 207.6444444444445]]\n"
     ]
    }
   ],
   "source": [
    "# MDP value iteration tests\n",
    "p, u = Problem(deterministic_test).solve(ITERATIONS, False)\n",
    "print(p)\n",
    "print(u)\n",
    "# print(p.best_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "txLGS4pUwhh7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "D D D L D\n",
      "R R L L L\n",
      "U U U U U\n",
      "U U U L U\n",
      "\n",
      "[[207.72758935374807, 243.86348225368286, 177.0497361881302, 154.78421657424204, 163.99373373654106], [285.36920834977286, 335.01148832783446, 506.09504346753283, 209.8445991422537, 225.2891015406941], [209.66137812077235, 246.1336692044712, 175.9051610269696, 154.66118749748017, 165.52039295955854], [179.68765223189604, 210.945771467252, 193.64607602495866, 156.56813818728887, 141.8571750027871]]\n"
     ]
    }
   ],
   "source": [
    "p, u = Problem(sampleMDP).solve(ITERATIONS, False)\n",
    "print(p)\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "D D D R D\n",
      "R L L L L\n",
      "D D D R D\n",
      "U U U R U\n",
      "\n",
      "[[176.5115520543013, 141.20924164344106, 5.499167388424542, 135.83226925188637, 169.79033656485797], [336.540040056473, 269.2320320451784, 508.4627507633712, 109.04718127195716, 323.7252519946153], [185.8562812807055, 148.6850250245644, 89.61382919134113, 143.02338938875093, 178.77923673593867], [232.32035160088185, 185.8562812807055, 299.5172864891764, 178.77923673593867, 223.47404591992336]]\n"
     ]
    }
   ],
   "source": [
    "p, u = Problem(very_slippy_test).solve(ITERATIONS, False)\n",
    "print(p)\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "INhKxA6twic8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R R R U L L R R U L\n",
      "R R U U U L U U U U\n",
      "U U U U U U U R U D\n",
      "R U U U U R D D U D\n",
      "R U U U R R R R R R\n",
      "\n",
      "[[607.3825503355703, 629.194630872483, 855.7046979865769, 1249.9999999999995, 855.7046979865769, 629.194630872483, 792.2946295903203, 879.8556569730415, 1249.9999999999995, 874.5489280407554], [484.2260402684563, 837.3557046979865, 684.5637583892616, 849.9999999999997, 684.5637583892616, 587.3557046979864, 633.8357036722563, 553.8845255784332, 999.9999999999997, 699.6391424326043], [276.79091543624156, 453.2730201342281, 602.4161073825502, 657.9999999999998, 452.4161073825502, 483.2730201342281, 407.7754192315855, 608.9599999999999, 729.9999999999998, 807.9999999999998], [237.90449310067112, 468.8719892617449, 521.9114093959731, 469.8399999999998, 299.911409395973, 476.03455999999994, 553.4719999999999, 646.3999999999999, 690.3999999999999, 849.9999999999997], [336.4820596144966, 400.83253605369123, 449.8624429530201, 451.4431999999998, 515.8031359999999, 594.0031999999999, 691.8399999999998, 807.9999999999998, 849.9999999999997, 1249.9999999999995]]\n"
     ]
    }
   ],
   "source": [
    "p, u = Problem(big_test).solve(ITERATIONS, False)\n",
    "print(p)\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "LfUJKMPtCRCs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R D U R D\n",
      "R R U U D\n",
      "R U U D L\n",
      "R U L L L\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q-learning tests\n",
    "# Set seed every time for consistent executions;\n",
    "# comment out to get different random runs\n",
    "random.seed(340)\n",
    "print(Problem(deterministic_test).solve(ITERATIONS, True))\n",
    "# print(Problem(deterministic_test).solve(10, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "08cHCoI6wqak"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "D D U R D\n",
      "R R U U D\n",
      "U U U D D\n",
      "U U L L L\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(340)\n",
    "print(Problem(sampleMDP).solve(ITERATIONS, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "PMM3kelxwqsx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "D L U R D\n",
      "R L U U L\n",
      "D L U R D\n",
      "U R U R U\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(340)\n",
    "print(Problem(very_slippy_test).solve(ITERATIONS, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "hWu_w30AwrP9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "L U R U L U R R U L\n",
      "U U U U U D U U U U\n",
      "U U U U U D U D U U\n",
      "U D U U U D D D L U\n",
      "R R U L L L L L U U\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(340)\n",
    "print(Problem(big_test).solve(ITERATIONS, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbP5VrvF9dIt"
   },
   "source": [
    "Once you're done, here are a few thought questions (19 points total):\n",
    "\n",
    "**3, 5 points) Suppose we are on the deterministic map where there is no sliding on ice, and performing value iteration until it converges.  Supposing 0 < DISCOUNT_FACTOR < 1, how does the policy change if the discount factor changes to another value in that range (or does the policy change at all)?  Why does that happen?  What happens to the policy if DISCOUNT_FACTOR = 1?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzD5X8wY-5gS"
   },
   "source": [
    "**TODO**\n",
    "Changes to the discount factor would change the policy by increasing the value the policy places on rewards further away if the discount factor increases, or decreasing the value the policy places on rewards further away. If the discount factor is equal to 1, the policy would find the action that maximizes the total reward/utility for each space, regardless of how efficient it is. This could lead to the policy recommending actions that take much more moves to reach the gold.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CkLwZnX-87e"
   },
   "source": [
    "**4, 3 points) The value iteration MDP solver updates all squares an equal number of times.  The Q-learner does not.  Which squares might we expect the Q-learner to update the most?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEhHogqO_7fC"
   },
   "source": [
    "**TODO**\n",
    "The squares we might expect the Q-learner to update the most would be the squares with gold. The reason we might expect the Q-learner to update the squares with gold the most is because the Q-learner emphasizes exploring/following paths with the highest reward that it has found. While every square could be explored by random choice, the Q-learner would learn about the positive value/utility of the gold spaces, causing the Q-learner to explore the paths leading to the gold spaces more than paths that lead to the pit spaces. Since each of the paths leading to the gold spaces ends with a gold space, the gold spaces would be updated the most by the Q-learner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DeGcHwKR_-m7"
   },
   "source": [
    "**5, 11 points) Suppose we change the state information so that, instead of knowing its coordinates on the map, the agent instead knows just the locations of all rewards in a 5x5 square with the agent at the square center.  Thus, at the start of every run, it may not know exactly where it is, but it knows what is in the vicinity.  It also does not know the transition model.**\n",
    "\n",
    "**a, 2 points) We can't use value iteration here.  Why?**\n",
    "\n",
    "**b, 4 points) How many state-action combinations are possible, assuming the contents of the agent's own square don't matter, and every other square could have a pit, gold, or an empty square as in the example maps?  Is a lookup table of Q-values feasible if we allocate memory for each possible state-action combination?  (Let's define \"feasible\" as \"able to be stored in a gig or less of memory,\" assuming 64-bit values.)**\n",
    "\n",
    "**c, 5 points) Let's suppose we want to instead generate Q-values with a classic neural network with a single hidden layer.  The inputs are the contents of the 24 squares in the 5x5 square that the player is not  in (we can encode gold = 1, nothing = 0, pit = -1).  There are 10 hidden units.  There are 4 output units corresponding to the 4 possible actions' Q-values.  How much memory is required for the weights of this network, assuming each is a 32-bit float (don't forget bias weights for each unit)?  Comparing to part (b), is it more efficient in memory to use a lookup table for Q(s,a), or this neural network?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Inb8brIUIk8U"
   },
   "source": [
    "**a) TODO**\n",
    "We can't use value iteration here because we don't know the transitional model. Therefore, we can't apply the Bellman equation to accurately calculate the value for each space using value iteration. However, it would be possible to utilize a Q-learner model to explore this problem since it doesn't need to know the transitional model to explore the problem.\n",
    "\n",
    "**b) TODO**\n",
    "The number of state-action combinations possible is the number of different states times the number of different actions. This would mean that the amount of memory required for the lookup table of Q-values would be $(5 * 5 - 1)  * 4 = 96$ values, each taking up 64 bits for a total of 6144 bits. This is because for each state (except for the agent's starting square), a Q learner would need to store the utility of every possible action. A lookup table of Q-values would be feasible if we allocate memory for each possible state-action combination.\n",
    "\n",
    "**c) TODO**\n",
    "The amount of memory required for the weights of this classic neural network with a single hidden layer assuming each weight is a 32 bit float would be 9408 bits. There are $(24+1) * 10 + (10 + 1) * 4 = 294$ weights in the neural network (each neuron has weights for each of its inputs + 1 for the bias weight). Since there are 294 weights, each taking up 32 bits, the total number of bits required to represent the weights of this classic neural network would be 9408. Comparing to part (b), the more efficient memory implementation would be to utilize a lookup table for Q(s, a)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78PjFoZBOLfp"
   },
   "source": [
    "**Remember to submit your code on Blackboard as both an .ipynb (File->Download->.ipynb) and a PDF (Print->Save as PDF).**"
   ]
  }
 ],
 "metadata": {
  "author": [
   {
    "@type": "Person",
    "name": "Kevin Gold"
   }
  ],
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
